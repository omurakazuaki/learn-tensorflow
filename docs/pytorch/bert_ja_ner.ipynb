{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8eb903e9-8f2f-48d3-bcaf-e7a5836b5a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fugashi in /usr/local/lib/python3.10/dist-packages (1.1.2)\n",
      "Requirement already satisfied: ipadic in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install fugashi ipadic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee1ac12f-b889-4fad-a0f1-ccba30f1ec1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import unicodedata\n",
    "import itertools\n",
    "from functools import reduce\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertJapaneseTokenizer, BertForTokenClassification\n",
    "\n",
    "# 日本語学習済みモデル\n",
    "MODEL_NAME = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
    "EPOCHS = 3\n",
    "MAX_WORDS_LEN = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5d69e7b-8d9e-408b-a195-1fa5c32e6c78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebe0d54c-3337-4603-b8b3-18b21dd93055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path '../data' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/stockmarkteam/ner-wikipedia-dataset ../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba7ffaf3-b7c1-4201-851d-978bc7a81d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'curid': '2415078',\n",
       " 'text': 'レッドフォックス株式会社は、東京都千代田区に本社を置くITサービス企業である。',\n",
       " 'entities': [{'name': 'レッドフォックス株式会社', 'span': [0, 12], 'type': '法人名'},\n",
       "  {'name': '東京都千代田区', 'span': [14, 21], 'type': '地名'}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# データのロード\n",
    "dataset = json.load(open('../data/ner-wikipedia-dataset/ner.json','r'))\n",
    "\n",
    "# 固有表現のタイプとIDを対応付る辞書 \n",
    "type_id_dict = {\n",
    "    \"人名\": 1,\n",
    "    \"法人名\": 2,\n",
    "    \"政治的組織名\": 3,\n",
    "    \"その他の組織名\": 4,\n",
    "    \"地名\": 5,\n",
    "    \"施設名\": 6,\n",
    "    \"製品名\": 7,\n",
    "    \"イベント名\": 8\n",
    "}\n",
    "type_name_dict = {v: k for k, v in type_id_dict.items()}\n",
    "\n",
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b194acb1-ee7a-4e6b-ac1c-6a0405cc4bfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'curid': '2415078',\n",
       " 'text': 'レッドフォックス株式会社は、東京都千代田区に本社を置くITサービス企業である。',\n",
       " 'entities': [{'name': 'レッドフォックス株式会社', 'span': [0, 12], 'type_id': 2},\n",
       "  {'name': '東京都千代田区', 'span': [14, 21], 'type_id': 5}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# カテゴリーをラベルに変更、文字列の正規化する。\n",
    "for sample in dataset:\n",
    "    sample['text'] = unicodedata.normalize('NFKC', sample['text'])\n",
    "    for e in sample[\"entities\"]:\n",
    "        e['type_id'] = type_id_dict[e['type']]\n",
    "        del e['type']\n",
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db0ea4d0-6039-416b-9150-6f58c065e360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train: 3500\n",
      "Length of val: 1000\n",
      "Length of test: 843\n"
     ]
    }
   ],
   "source": [
    "# データセットの分割\n",
    "n = len(dataset)\n",
    "n_train = int(3500)\n",
    "n_val = int(1000)\n",
    "dataset_train = dataset[:n_train]\n",
    "dataset_val = dataset[n_train:n_train+n_val]\n",
    "dataset_test = dataset[n_train+n_val:]\n",
    "\n",
    "print(f\"Length of train: {len(dataset_train)}\")\n",
    "print(f\"Length of val: {len(dataset_val)}\")\n",
    "print(f\"Length of test: {len(dataset_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "421b0784-501e-4b03-9543-a267313db958",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NerTokenizerForTrain(BertJapaneseTokenizer):\n",
    "\n",
    "  def create_tokens_and_labels(self, splitted):\n",
    "      \"\"\"分割された文字列をトークン化し、ラベルを付与\n",
    "      Args：\n",
    "        splitted: 分割された文字列\n",
    "          例：\n",
    "          [{'text': 'レッドフォックス株式会社', 'label': 2},\n",
    "          {'text': 'は、', 'label': 0},\n",
    "          {'text': '東京都千代田区', 'label': 5},\n",
    "          {'text': 'に本社を置くITサービス企業である。', 'label': 0}]\n",
    "      Return:\n",
    "        tokens, labels\n",
    "          例：\n",
    "          ['レッド', 'フォックス', '株式会社', 'は', '、', '東京', '都', '千代田', '区', 'に', '本社', 'を', '置く', 'IT', 'サービス', '企業', 'で', 'ある', '。']\n",
    "          [2, 2, 2, 0, 0, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "      \"\"\"\n",
    "      tokens = [] # トークン格納用\n",
    "      labels = [] # トークンに対応するラベル格納用\n",
    "      for s in splitted:\n",
    "          text = s['text']\n",
    "          label = s['label']\n",
    "          tokens_splitted = self.tokenize(text) # BertJapaneseTokenizerのトークナイザを使ってトークンに分割\n",
    "          labels_splitted = [label] * len(tokens_splitted)\n",
    "          tokens.extend(tokens_splitted)\n",
    "          labels.extend(labels_splitted)\n",
    "      \n",
    "      return tokens, labels\n",
    "\n",
    "\n",
    "  def encoding_for_bert(self, tokens, labels, max_length):\n",
    "      \"\"\"符号化を行いBERTに入力できる形式にする\n",
    "      Args:\n",
    "        tokens: トークン列\n",
    "        labels: トークンに対応するラベルの列\n",
    "      Returns: \n",
    "        encoding: BERTに入力できる形式\n",
    "        例：\n",
    "        {'input_ids': [2, 3990, 13779, 1275, 9, 6, 391, 409, 9674, 280, 7, 2557, 11, 3045, 8267, 1645, 1189, 12, 31, 8, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \n",
    "        　'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \n",
    "        　'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]},\n",
    "          'labels': [0, 2, 2, 2, 0, 0, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
    "\n",
    "      \"\"\"\n",
    "      encoding = self.encode_plus(\n",
    "          tokens, \n",
    "          max_length=max_length, \n",
    "          padding='max_length', \n",
    "          truncation=True\n",
    "      ) \n",
    "      # トークン[CLS]、[SEP]のラベルを0\n",
    "      labels = [0] + labels[:max_length-2] + [0] \n",
    "      # トークン[PAD]のラベルを0\n",
    "      labels = labels + [0]*( max_length - len(labels) ) \n",
    "      encoding['labels'] = labels\n",
    "\n",
    "      return encoding\n",
    "\n",
    "\n",
    "  def encode_plus_tagged(self, text, entities, max_length):\n",
    "      \"\"\"文章とそれに含まれる固有表現が与えられた時に、符号化とラベル列の作成\n",
    "      Args:\n",
    "        text: 元の文章\n",
    "        entities: 文章中の固有表現の位置(span)とラベル(type_id)の情報\n",
    "\n",
    "      \"\"\"\n",
    "      # 固有表現の前後でtextを分割し、それぞれのラベルをつけておく。\n",
    "      entities = sorted(entities, key=lambda x: x['span'][0]) # 固有表現の位置の昇順でソート\n",
    "      splitted = [] # 分割後の文字列格納用\n",
    "      position = 0\n",
    "      for entity in entities:\n",
    "          start = entity['span'][0]\n",
    "          end = entity['span'][1]\n",
    "          label = entity['type_id']\n",
    "          # 固有表現ではないものには0のラベルを付与\n",
    "          splitted.append({'text': text[position:start], 'label':0}) \n",
    "          # 固有表現には、固有表現のタイプに対応するIDをラベルとして付与\n",
    "          splitted.append({'text': text[start:end], 'label':label}) \n",
    "          position = end\n",
    "\n",
    "      # 最後の固有表現から文末に、0のラベルを付与\n",
    "      splitted.append({'text': text[position:], 'label':0})\n",
    "      # positionとspan[0]の値が同じだと空白文字にラベル0が付与されるため、長さ0の文字列は除く（例：{'text': '', 'label': 0}）\n",
    "      splitted = [ s for s in splitted if s['text'] ] \n",
    "\n",
    "      # 分割された文字列をトークン化し、ラベルを付与\n",
    "      tokens, labels = self.create_tokens_and_labels(splitted)\n",
    "\n",
    "      # 符号化を行いBERTに入力できる形式にする\n",
    "      encoding = self.encoding_for_bert(tokens, labels, max_length)\n",
    "\n",
    "      return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a144e75f-305f-433d-b305-7f1e959b4160",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
      "The class this function is called from is 'NerTokenizerForTrain'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'curid': '2415078',\n",
      " 'entities': [{'name': 'レッドフォックス株式会社', 'span': [0, 12], 'type_id': 2},\n",
      "              {'name': '東京都千代田区', 'span': [14, 21], 'type_id': 5}],\n",
      " 'text': 'レッドフォックス株式会社は、東京都千代田区に本社を置くITサービス企業である。'}\n",
      "{'attention_mask': [1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0],\n",
      " 'input_ids': [2,\n",
      "               3990,\n",
      "               13779,\n",
      "               1275,\n",
      "               9,\n",
      "               6,\n",
      "               391,\n",
      "               409,\n",
      "               9674,\n",
      "               280,\n",
      "               7,\n",
      "               2557,\n",
      "               11,\n",
      "               3045,\n",
      "               8267,\n",
      "               1645,\n",
      "               1189,\n",
      "               12,\n",
      "               31,\n",
      "               8,\n",
      "               3,\n",
      "               0,\n",
      "               0,\n",
      "               0,\n",
      "               0,\n",
      "               0,\n",
      "               0,\n",
      "               0,\n",
      "               0,\n",
      "               0,\n",
      "               0,\n",
      "               0,\n",
      "               0,\n",
      "               0,\n",
      "               0,\n",
      "               0,\n",
      "               0,\n",
      "               0,\n",
      "               0,\n",
      "               0,\n",
      "               0,\n",
      "               0,\n",
      "               0,\n",
      "               0,\n",
      "               0,\n",
      "               0,\n",
      "               0,\n",
      "               0,\n",
      "               0,\n",
      "               0,\n",
      "               0,\n",
      "               0,\n",
      "               0,\n",
      "               0,\n",
      "               0,\n",
      "               0,\n",
      "               0,\n",
      "               0,\n",
      "               0,\n",
      "               0,\n",
      "               0,\n",
      "               0,\n",
      "               0,\n",
      "               0],\n",
      " 'labels': [0,\n",
      "            2,\n",
      "            2,\n",
      "            2,\n",
      "            0,\n",
      "            0,\n",
      "            5,\n",
      "            5,\n",
      "            5,\n",
      "            5,\n",
      "            0,\n",
      "            0,\n",
      "            0,\n",
      "            0,\n",
      "            0,\n",
      "            0,\n",
      "            0,\n",
      "            0,\n",
      "            0,\n",
      "            0,\n",
      "            0,\n",
      "            0,\n",
      "            0,\n",
      "            0,\n",
      "            0,\n",
      "            0,\n",
      "            0,\n",
      "            0,\n",
      "            0,\n",
      "            0,\n",
      "            0,\n",
      "            0,\n",
      "            0,\n",
      "            0,\n",
      "            0,\n",
      "            0,\n",
      "            0,\n",
      "            0,\n",
      "            0,\n",
      "            0,\n",
      "            0,\n",
      "            0,\n",
      "            0,\n",
      "            0,\n",
      "            0,\n",
      "            0,\n",
      "            0,\n",
      "            0,\n",
      "            0,\n",
      "            0,\n",
      "            0,\n",
      "            0,\n",
      "            0,\n",
      "            0,\n",
      "            0,\n",
      "            0,\n",
      "            0,\n",
      "            0,\n",
      "            0,\n",
      "            0,\n",
      "            0,\n",
      "            0,\n",
      "            0,\n",
      "            0],\n",
      " 'token_type_ids': [0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                    0]}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = NerTokenizerForTrain.from_pretrained(MODEL_NAME)\n",
    "import pprint\n",
    "tmp = dataset_train[1]\n",
    "pprint.pprint(tmp)\n",
    "pprint.pprint(tokenizer.encode_plus_tagged(text=tmp[\"text\"], entities=tmp[\"entities\"], max_length=MAX_WORDS_LEN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f534537-79b0-484f-a08f-05475e45efce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   2,  408,    5,  ...,    0,    0,    0],\n",
       "         [   2,  521,    6,  ...,    0,    0,    0],\n",
       "         [   2, 4006,    6,  ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [   2, 1526,   19,  ...,    0,    0,    0],\n",
       "         [   2,   70,   19,  ...,    0,    0,    0],\n",
       "         [   2,  960,   19,  ...,    0,    0,    0]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'labels': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def collate_batch(batch):\n",
    "    encoding = [tokenizer.encode_plus_tagged(text=ele[\"text\"], entities=ele[\"entities\"], max_length=MAX_WORDS_LEN) for ele in batch]\n",
    "\n",
    "    def reducer(acc, curr):\n",
    "        for key, value in curr.items():\n",
    "            if key not in acc:\n",
    "                acc[key] = [value]\n",
    "            else:\n",
    "                acc[key].append(value)\n",
    "        return acc\n",
    "\n",
    "    def toTensor(acc, curr):\n",
    "        acc[curr[0]] = torch.tensor(curr[1])\n",
    "        return acc\n",
    "\n",
    "    return reduce(toTensor, reduce(reducer, encoding, {}).items(), {})\n",
    "    \n",
    "# データローダーの作成\n",
    "train_dataloader = DataLoader(dataset_train, batch_size=35, shuffle=True, pin_memory=True, collate_fn=collate_batch)\n",
    "val_dataloader = DataLoader(dataset_val, batch_size=100, shuffle=True, pin_memory=True, collate_fn=collate_batch)\n",
    "train_len = len(dataset_train)\n",
    "test_len = len(dataset_val)\n",
    "\n",
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cd8b609-9181-480d-a6e6-3dd8d36322a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 学習済みモデルのロード\n",
    "model = BertForTokenClassification.from_pretrained(MODEL_NAME, num_labels=9).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8a7048a-b8f8-4ae3-89d4-714057e2effd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1(100.00%) | Train Loss: 0.352 | Elapsed: 25.1s\n",
      "epoch:1(100.00%) | Train Loss: 0.060 | Elapsed: 2.4s\n",
      "epoch:2(100.00%) | Train Loss: 0.047 | Elapsed: 24.7s\n",
      "epoch:2(100.00%) | Train Loss: 0.048 | Elapsed: 2.5s\n",
      "epoch:3(100.00%) | Train Loss: 0.023 | Elapsed: 25.0s\n",
      "epoch:3(100.00%) | Train Loss: 0.047 | Elapsed: 2.5s\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=2e-5)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "for epoch_num in range(EPOCHS):\n",
    "\n",
    "    total_loss_train = []\n",
    "    total_loss_val = []\n",
    "\n",
    "    time_s = time.perf_counter()\n",
    "    model.train()\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss, logits = model(input_ids=input_ids, \n",
    "                              token_type_ids=None, \n",
    "                              attention_mask=attention_mask, \n",
    "                              labels=labels,\n",
    "                              return_dict=False)\n",
    "\n",
    "        total_loss_train.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"epoch:{epoch_num+1}({(i + 1) / (train_len / train_dataloader.batch_size):.2%}) | Train Loss: {np.mean(total_loss_train):.3f} | Elapsed: {(time.perf_counter() - time_s):.1f}s\", end='\\r')\n",
    "\n",
    "    print()\n",
    "    time_s = time.perf_counter()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(val_dataloader):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            loss, logits = model(input_ids=input_ids, \n",
    "                                  token_type_ids=None, \n",
    "                                  attention_mask=attention_mask, \n",
    "                                  labels=labels,\n",
    "                                  return_dict=False)\n",
    "    \n",
    "            total_loss_val.append(loss.item())\n",
    "\n",
    "            print(f\"epoch:{epoch_num+1}({(i + 1) / (test_len / val_dataloader.batch_size):.2%}) | Train Loss: {np.mean(total_loss_val):.3f} | Elapsed: {(time.perf_counter() - time_s):.1f}s\", end='\\r')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "417292ca-a07d-48ec-a70e-7823d4c4cf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, '../models/bert_jp_ner.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffc2e39d-385b-4a3d-8b79-6399b7e17e83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'レッドフォックス株式会社は、東京都千代田区に本社を置くITサービス企業である。',\n",
       " 'entities': [{'name': 'レッドフォックス株式会社', 'span': [1, 4], 'type_id': '法人名'},\n",
       "  {'name': '東京都千代田区', 'span': [6, 10], 'type_id': '地名'}]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load('../models/bert_jp_ner.pth')\n",
    "def predict(text):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        encoded = collate_batch([{'text': text, 'entities': []}])\n",
    "        input_ids = encoded[\"input_ids\"].to(device)\n",
    "        attention_mask = encoded[\"attention_mask\"].to(device)\n",
    "        labels = encoded[\"labels\"].to(device)\n",
    "        loss, logits = model(input_ids=input_ids, \n",
    "                              token_type_ids=None, \n",
    "                              attention_mask=attention_mask, \n",
    "                              labels=labels,\n",
    "                              return_dict=False)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    pred_labels = logits.argmax(dim=2).tolist()[0]\n",
    "    entities = []\n",
    "    pos = 0\n",
    "    for label, group in itertools.groupby(pred_labels):\n",
    "        end = pos + len(list(group))\n",
    "        if (label != 0):\n",
    "            entity = {\n",
    "                \"name\": \"\".join(tokens[pos:end]),\n",
    "                \"span\": [pos, end],\n",
    "                \"type_id\": type_name_dict[label]\n",
    "            }\n",
    "            entities.append(entity)\n",
    "        pos = end\n",
    "        \n",
    "    return { 'text': text, 'entities': entities }\n",
    "\n",
    "result = predict('レッドフォックス株式会社は、東京都千代田区に本社を置くITサービス企業である。')\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
